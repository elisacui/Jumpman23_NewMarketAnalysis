---
title: "New Market Analysis - Jumpman23"
author: "Elisa Cui"
date: "7/28/2020"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
    self_contained: true
    toc: true
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE, echo=FALSE}
library(lubridate)
library(ggmap)
library(Rssa)
library(ggplot2)
library(readr)
library(tidyverse)
library(dplyr)
library(chron)
library(geosphere)
library(leaflet)
library(gridExtra)
library(tibble)
library(reshape2)
library(anytime)
require(devtools)
devtools::install_github("dkahle/ggmap", ref = "tidyup")
library(ggrepel)
library(data.table)
library(forecast)
library(fpp2)
library(TTR)
library(prettydoc)
```

### Challenge
How are things in New York for Jumpman23? Come up with a plan to grow the market by 20% in two months. Dive into the reports on data integrity issues and if they exist, outline where and how they may impact the analysis.

### Proposed Solution 
* Fix data integrity bugs by creating an incentive called a "perfect delivery" for Jumpman deliverers to keep track of their progress, note their arrival/departure time and ensure accurate delivery times. By using the data from a "perfect delivery," we can further determine accurate delivery times to ensure all businesses uphold a good reputation.
* Keep track of item attributes if an order is complete by matching up empty fields with receipts, then later using that data for businesses to recognize the popularity of their items to avoid null data.
* Divide the customers into different classes based off their history of ordering with Jumpman and create incentives on a level by level basis to gain customer retention.
* By using a customer's own data to determine the best time to send out an incentive to increase existing customer engagement and predict customer's future actions.
* Expand into upper east side of NY in order to expand entire city of NY since majority of orders are concentrated in lower manhattan & east village.
* Creat new customer acquistion campaigns that will further grow new customer base while retaining unique customer growth.

### Understanding & Transforming the Dataset
Jumpman23 is an on-demand delivery platform connection "Jumpmen" to customers purchasing a variety of goods. Jumpman23 will send Jumpmen to merchants to purchase and pickup any items requested by the customer. Based off initial data screening, I have transformed certain variables for later use and recognized potential data integrity issues.
```{r results = "hide"}
data <- read.csv(file = "~/Desktop/Jumpman23/analyze_me.csv", header = TRUE, na.strings=c("","NA"))
glimpse(data)
#18 columns & 5983 obs
#3 columns used for identification of order: delivery_id, customer_id, jumpman_id
#transforming data and using haversine formula to determine distance of pickup and dropoff
data <- data %>% rowwise() %>% mutate(distance = distHaversine(c(pickup_lon, pickup_lat), c(dropoff_lon, dropoff_lat))) 
#creating variables and transforming time data
#start to pickup: delivery started minus jumpman arrived at pickup
#start to end: total time of delivery
data$start_to_pickup <- difftime(as.POSIXct(data$when_the_Jumpman_arrived_at_pickup, format = "%Y-%m-%d %H:%M:%S"), as.POSIXct(data$when_the_delivery_started, format = "%Y-%m-%d %H:%M:%S"))
data$start_to_end <- difftime(as.POSIXct(data$when_the_Jumpman_arrived_at_dropoff, format = "%Y-%m-%d %H:%M:%S"), as.POSIXct(data$when_the_delivery_started, format ="%Y-%m-%d %H:%M:%S"))
#distance_miles: distance for each pickup and dropoff
data$distance_miles <- (data$distance / 1609)
#which_day: for determining out which day of the week people order from the most
#which_hour: for determining which hour of the day people order the most
data$which_day <- weekdays(as.Date(data$when_the_delivery_started))
data$which_hour <- format(as.POSIXct(data$when_the_delivery_started, format ="%Y-%m-%d %H:%M:%S"), "%H")
```
### Data Integrity Issues  
There are a few columns have missing values in the dataset, which are:  
  
* place_category: 883  
* item_name: 1230  
* item_quantity: 1230  
* item_category_name: 1230  
* how_long_it_took_to_order: 2945  
* when_the_jumpman_arrived_at_pickup: 550  
* when_the_jumpman_left_pickup: 550  
  
In most cases, we would be able to drop the missing values if the number of cases is less than 5% of the sample. If I were to omit all the rows with missing values, it would deduct the dataset by 61.9% which is a dramatic decrease in the number of observations given. Therefore, the best method to move forward is to judge the missing information by their local category. For example, looking at the item's name, quantitiy and category, there is a correlation in which if one variable is null, it will leave the next variable null and so on. The best method fo is to parse the missing values by their local category. These are the categories I have separated them into:  
  
* item_quantity vs. empty_quantity  
* pickup_time vs. empty_pickup_time   
* place_category vs. empty_place_category  
* time_to_order vs. empty_time_to_order  

```{r results = "hide"}
#counting all the complete rows within the dataset to see the overall data integrity issue's impact
#only 2279 rows have complete cases within the dataset
jumpman.data <- data[complete.cases(data),]
head(jumpman.data)

#count empty place category rows
empty_place_category <- data[is.na(data$place_category),]
count(empty_place_category)

#count missing jumpman arrival/pickup (correlated data)
missing_when_jumpman_arrived_at_pickup <- data[is.na(data$when_the_Jumpman_arrived_at_pickup),]
count(missing_when_jumpman_arrived_at_pickup)

#count missing how lont it took to order 
miss_how_long_it_took_to_order <- data[is.na(data$how_long_it_took_to_order),]
count(miss_how_long_it_took_to_order)

```

#### item_quantity vs. empty_quantity
#### Problem  
About 20.5% of the dataset has missing information about the items ordered. This complicates things for the Jumpman because he/she could run into confusion when no information about the items purchased are listed. TSince these fields are related to one another, I will be splitting the dataset based on item_quantity. If this field is left blank, there is no clear KPI that the item is doing well which directly affects the business working with Jumpman23.

#### Explaination 1
There is a correlation between shorter delivery distanace and missing items in the dataset. From this graph, it is possible that customers could have changed their minds about ordering something due to the proximity of where the customer is and the place they are ordering from.  
```{r include = FALSE, warning=FALSE, message=FALSE}
#creating df for item_quantity in which the column has no missing values
data$item_quantity <- as.numeric(data$item_quantity)
item_quantity <- data[!is.na(data$item_quantity),]

#creating df for empty_quantity in which the column has all missing values
empty_quantity <- data[is.na(data$item_quantity),]
count(empty_quantity)
```
```{r}
ggplot() + geom_density(data = item_quantity, aes(x = distance_miles, fill = "Item Noted"), adjust = 1, alpha = .3) + geom_density(data = empty_quantity, aes(x = distance_miles, fill = "N/A"), adjust = 1, alpha = .3) + scale_fill_manual(name = "Item Quantity", values = c("Item Noted" = "#1A97DB", "N/A" = "#E68877")) + labs(x = "Miles Traveled", y = "Density", title = "Delivery Distance vs. Null Item") 
```
#### Explaination 2  
By determining if this is related to other attributes within the dataset, it is valid that missing item values are not correlated with other attributes such as jumpman_id, customer_id, pickup_place, vehicle_type or place_category. Separating this issue from other attributes, we can further investigate the root of the problem in regard's to a customer's behavior when it comes to ordering from Jumpman.
```{r warning=FALSE, message=FALSE}
#querying for results of any correlation within the two datasets of item_quantity & empty_quantity
#all results came out positive which means there is no unique row in which null values for item quantity correlate to the attributes listed below
nrow(item_quantity[which(item_quantity$jumpman_id %in% unique(empty_quantity$jumpman_id)), ])
nrow(item_quantity[which(item_quantity$customer_id %in% unique(empty_quantity$customer_id)), ])
nrow(item_quantity[which(item_quantity$pickup_place %in% unique(empty_quantity$pickup_place)), ])
nrow(item_quantity[which(item_quantity$vehicle_type %in% unique(empty_quantity$vehicle_type)), ])
nrow(item_quantity[which(item_quantity$place_category %in% unique(empty_quantity$place_category)), ])
```
#### Solution
If the transaction is completed, meaning the Jumpman successfully delivered the items to the customer from the pickup location, and the item's name, category, quantity are missing; under both these conditions, Jumpman23 can create application that checks for missing data by comparing the empty field to the receipt of the customer and filling out the data accordingly due to their receipt. This way Jumpman23's database will not be negatively impacted by orders with missing items.

#### pickup_time vs. empty_pickup_time  
#### Problem  
Missing arrival and departure time of the Jumpman from the pickup location accounts for 9.1% of the dataset. This information correlates with determining the estimated time of arrival of the Jumpman to the customer. With this field missing, it can complicate the Jumpman's rating and efficiency while impacting the location's actual estimated delivery time. A customer's decision will be based off of what they see as most efficient, therefore, if the delivery will take longer based on past deliverers not tracking down their time, it could potentially affect the business.

#### Explaination 1
The Jumpman forgets to open their phone and note their arrival and departure time at the pickup location. Based off of the median calculations, it is important to note that when a Jumpman does not report their time of arrival/departure from the pickup, it is not only shorter distance but also longer delivery time, meaning there is some ambiguity between the Jumpman forgetting to note their time and the distance of the delivery.

```{r}
#count missing jumpman arrival/pickup (correlated data so i will only be using the dataset of arrival time for pickup)
pickup_time <- data[!is.na(data$when_the_Jumpman_arrived_at_pickup),]
empty_pickup_time <- data[is.na(data$when_the_Jumpman_arrived_at_pickup),]
#creating variable for the time it takes for a delivery starting to when the jumpman brings it to the customer
empty_pickup_time$start_to_pickup <- difftime(as.POSIXct(empty_pickup_time$when_the_Jumpman_arrived_at_pickup, format = "%Y-%m-%d %H:%M:%S"), as.POSIXct(empty_pickup_time$when_the_delivery_started, format = "%Y-%m-%d %H:%M:%S"))
#creating variable for the time it takes for a delivery starting to when the jumpman brings it to the customer
empty_pickup_time$start_to_end <- difftime(as.POSIXct(empty_pickup_time$when_the_Jumpman_arrived_at_dropoff, format = "%Y-%m-%d %H:%M:%S"), as.POSIXct(empty_pickup_time$when_the_delivery_started, format ="%Y-%m-%d %H:%M:%S"))

median(pickup_time$distance_miles)
median(empty_pickup_time$distance_miles)
median(pickup_time$start_to_end)
median(empty_pickup_time$start_to_end)

```
#### Explaination 2 
In some cases, the arrival time is earlier than when the delivery started, which resulted in negative time for calculating the amount of time it takes for the jumpman to go to the pickup location. These false values account for 497 observations which is 9.1% of the dataset.
```{r warning=FALSE, message=FALSE}

#creating variable for the time it takes for a delivery starting to when the jumpman picks it up
pickup_time$start_to_pickup <- difftime(as.POSIXct(pickup_time$when_the_Jumpman_arrived_at_pickup, format = "%Y-%m-%d %H:%M:%S"), as.POSIXct(pickup_time$when_the_delivery_started, format = "%Y-%m-%d %H:%M:%S"))
#creating variable for the time it takes for a delivery starting to when the jumpman brings it to the customer
pickup_time$start_to_end <- difftime(as.POSIXct(pickup_time$when_the_Jumpman_arrived_at_dropoff, format = "%Y-%m-%d %H:%M:%S"), as.POSIXct(pickup_time$when_the_delivery_started, format ="%Y-%m-%d %H:%M:%S"))

sum(pickup_time$start_to_pickup < 0)
```
#### Soultion  
Based off of these two explainations, I can present 3 solutions that could potentially fix this issue:  
  
1) Create an incentive called a "perfect delivery" in which the Jumpman is incentivised to record their arrival/departure time at the pickup location in order to complete a "perfect delivery". In return, this can be a streak or tally mark of how many perfect deliveries this Jumpman has completed, which in return, can ensure trust within the customer that their Jumpman is reliable and efficent. Something to note would be that this is not an incentive based on the quickness of the delivery; it is based on the Jumpman solely being able to note these times so that data and software engineers have reliable data to determine the estimated time of a delivery.
  
2) By using existing data of a "perfect delivery," it can track the reasonable amount of time it takes for a Jumpman to get from point A to point B based off of other perfect deliveries via that location. The Jumpman can aim to deliver in a certain amount of time since a "perfect delivery" is not based on how quick, but how cautious the Jumpman is. This will also make the estimation of the delivery more realistic, therefore giving customers a more accurate time for when they decide to order. This will also increase the overall efficiency of the Jumpmen.
  
3) By using tracking capabilities within the phone, when a Jumpman is about 1 mile away from the pickup location, a timestamp could be noted as a backup timestamp if the Jumpman forgets to note down their delivery time. Also, when the Jumpman is further away from the pickup location (after an appropriate amount of time since being 1 mile away from the pickup location), that time could also be noted to use as a backup timestamp. Whenever a Jumpman forgets to note those times, there will be estimations to fall back on for further predictions. 

#### More Data Integrity Issues  
There are two more problems within the dataset regarding data integrity that I will not go into further analysis but would like to touch on quickly. 
  
* Empty values for place_category: This is presumably due to when a business is being onboarded by Jumpman23, the business owner or the customer success agent could have forgotten to include the place category. If the location has a name, the category should be a required field to be entered and stored into the API. This is important because some businesses could be lost in the directory when a customer is looking to purchase by category. As someone who frequently uses delivery services, I can strongly say that when I am in an indecisive mood, I will be looking through the categories to figure out what I'm in the mood for.   
  
* Empty values for how_long_it_took_to_order: This has missing values potentially due to the fact that customers could sometimes log in and out of the application due to decision fatigue. If a customer was distracted or another event came up while they were trying to order in the application, it is difficult to track exactly how much time it took for them to order. A quick solution to this could be to only track the time of a customer when they have selected an item for their shopping cart, if that shopping cart is inactive for 30 minutes (this is just an estimation, but this time could be predicted by behavioral data scientists), the timer that is counting time will reset due to inactivity. The timer will finish only when the purchase is completed and therefore that column will not be populated with a value.  
  
### Growth and Expansion  
In order to give the most accurate analysis possible, I have removed the rows where item quantity is not populated and arrival time at the pickup location is not before the delivery start time.  
```{r warning=FALSE, message=FALSE}
#removing rows without item quantity and rows where time to pick up is negative 
cleaned.data <- data[which(!is.na(data$item_quantity) & data$start_to_pickup > 0),]
```

#### Incentive Based on Peak Hours/Days
```{r warning=FALSE, message=FALSE}
order_by_day_hour <- cleaned.data %>% group_by(which_day, which_hour) %>% dplyr::summarise(count = n())

#delivery by time and day
ggplot(order_by_day_hour, aes(x = which_hour, y = which_day, fill = count)) + geom_tile() + labs(x = "Hour of Day", y = "Day of Week", title = "Number of Deliveries, by Day and Hour")
```

#### Existing Customer Retention
1) Classify the customers into different types of buyers based on their history - daily, weekly, bi-weekly or monthly customers. With the customers classified based on their previous track record, we can use this to determine how big of an incentive for each level of buyer.  
  
2) Based off the dataset, most popular items were fries and cheese fries. Both of these are natural cravings that people have and due to it being a side item, it is not as pricey as an entry. For creating an incentive, a daily Jumpman user will be excited to see a 10% off coupon while a monthly Jumpman user would be tempted by a 50% off coupon. In order to lure the customer into a purchase, creating deadlines and sending the promotion at the right time can really impact how a customer feels when they see the notification. Something clever would be to send out a notification based on each customer's individual usage of the application, i.e. if a customer is frequently opening the application at night or frequently opening it during lunchtime, send out the promotion during that specific customer's peak usage hour.  
```{r warning=FALSE, message=FALSE}
#which hour is the most popular
order_by_hour <- cleaned.data %>% group_by(which_hour, delivery_id) %>% dplyr::summarise(count = n())
ggplot(order_by_hour, aes(x = which_hour)) + geom_bar(fill = "#A15FE8") + geom_text(stat = 'count', aes(label=..count..), vjust = -1) + labs(title = "Orders by Hour", x = "Hour of the Day", y = "Orders") + ylim(0,600)
```
```{r warning=FALSE, message=FALSE}
#which day is the most popular
order_by_day <- cleaned.data %>% group_by(which_day, delivery_id) %>% dplyr::summarise(count = n())

ggplot(order_by_day, aes(x = which_day)) + geom_bar(fill = "#0073C2FF") + geom_text(stat = 'count', aes(label=..count..), vjust = -1) + labs(title = "Orders by Day", x = "Day of the Week", y = "Orders") + ylim(0,700)
```
  
3) If an area is heavily concentrated with customers who are in the monthly category, specifically lower the prices of the surrounding businesses to gain more attraction for those businesses while bringing more engagement to the application since lower prices = increase in customer engagement. 
  
4) This method is to solely maintain customer retention and grow dollar amount per purchase by using the customer's track record to market the correct incentive. By turning monthly customers into bi-weekly customers and bi-weekly into weekly, theoritically, this will grow the market and welcome a new level of buyers to the playing field (new customers).  
  
```{r warning=FALSE, message=FALSE}
#format delivery start time to only have the date
cleaned.data$only_date <- format(as.POSIXct(cleaned.data$when_the_delivery_started, format="%Y-%m-%d %H:%M:%S"), "%Y-%m-%d")

daily_order <- cleaned.data %>% group_by(only_date, delivery_id) %>% dplyr::summarise(count = n())

ggplot(daily_order, aes(x = as.Date(only_date), y = count)) + geom_jitter() + labs(x = "Date", y = "Number of Deliveries", title = "Deliveries per Day") + geom_smooth(colour="#42f5e3", size=1.5, method="loess", se=FALSE) + geom_smooth(method="lm", se=FALSE)
```
  
Deliveries are stable with modest growth, peaks within the graph occur mostly on sundays (as seen in the orders by day plot). 

#### New Customer Acquisition 
It is clear that 47.5% of orders occur on the weekend (friday, saturday, sunday) and 56.7% of the orders occur during the peak hours of 5PM - 9PM. Meanwhile, majority of the pickups happen within lower Manhattan, East Village and NoHo and then dropped off at homes that are a few miles outside of the concentrated areas.   
```{r warning=FALSE, message=FALSE}
register_google(key = "AIzaSyCDnYvyOg7yNC_znzzIZC64cXHdRKP6ujI")
ny_pickup <- ggmap(get_map(location=c(lon = -73.972026, lat = 40.745362), zoom=13, scale=4)) + stat_density2d(data = cleaned.data, aes(x =pickup_lon, y = pickup_lat, fill = ..level.., alpha = ..level..), geom = "polygon") 
ny_pickup + ggtitle("Map of Pickup Locations") 

ny_dropoff <- ggmap(get_map(location=c(lon = -73.972026, lat = 40.745362), zoom=13, scale=4)) + stat_density2d(data = cleaned.data, aes(x =dropoff_lon, y = dropoff_lat, fill = ..level.., alpha = ..level..), geom = "polygon") 
ny_dropoff + ggtitle("Map of Drop Off Locations") 
```

1) Based off of the heatmap for pickup locations, there is barely any action going on for the businesses in the upper east side, yet drop offs are being made in that area. In order to expand this, we would want to bring popular businesses within that area on board, which will bring new customers on board as well. High drop off location density implies that customers in that area could have a higher income, leading to more frequent use of delivery services.  

2) In order to grow the sector of new customers, it is crucial to have an easy methodology for a new user who is interested in the platform. Interconnected accounts and referrals can intiate to a new user to sign up and recieve an incentive for doing so. Sometimes, most people use the application once and never again, which is why having a strong incentive to lure them into reoccuring usage is important. This is a continuous cycle for customers since the more people are reminded of the capabilities of Jumpman, the more likely they will think to go to the application when they are in need of the service. 
```{r warning=FALSE, message=FALSE}

customer <- cleaned.data %>% select(customer_id, only_date) 
customer_by_day <- customer %>% group_by(only_date) %>% dplyr::summarise(count = n())

ggplot(customer_by_day, aes(x = as.Date(only_date), y = count)) + geom_point() + labs(x="Date", y="Number of Customers", title="Number of Unique Customers Ordering per Day") + geom_smooth(colour="#e642f5", size=1.5, method="loess", se=FALSE) + geom_smooth(method="lm", se=FALSE)

```
  
Unique customers are growing at a steady rate while the number of new customers have decreased, which means we need to pour more resources into increasing new customer acquisition. 
  
### Conclusion
With the proposed solutions to better the company's data integrity and expand growth, Jumpman23 should see an increase in productivity, existing customer retention and new customer acquisition for an overall growth of 20% or more. 

Thank you for your time. :-)
